{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Weights and Biases\n",
    "Install wandb, create an account, and set up API keys for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the `wandb` library\n",
    "!pip install wandb\n",
    "\n",
    "# Import the `wandb` library\n",
    "import wandb\n",
    "\n",
    "# Log in to Weights and Biases\n",
    "# This will prompt the user to enter their API key for authentication\n",
    "wandb.login()\n",
    "\n",
    "# Instructions for creating an account and setting up API keys\n",
    "# (This is a markdown cell in the notebook, not Python code)\n",
    "\"\"\"\n",
    "### Instructions:\n",
    "1. If you don't already have a Weights and Biases account, create one at [wandb.ai](https://wandb.ai).\n",
    "2. After creating an account, navigate to your account settings to find your API key.\n",
    "3. Run the `wandb.login()` command above and paste your API key when prompted.\n",
    "4. Once authenticated, you can start using Weights and Biases in your projects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Experiment Tracking\n",
    "Initialize wandb, configure projects, and learn how to track basic runs with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new W&B run\n",
    "wandb.init(\n",
    "    project=\"mlops-introduction\",  # Set the project name\n",
    "    name=\"basic-experiment-tracking\",  # Name of the run\n",
    "    config={  # Define default hyperparameters\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10\n",
    "    }\n",
    ")\n",
    "\n",
    "# Access the configuration\n",
    "config = wandb.config\n",
    "\n",
    "# Simulate a basic training loop and log metrics\n",
    "import time\n",
    "import random\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    # Simulate training and validation loss\n",
    "    train_loss = random.uniform(0.2, 1.0) / (epoch + 1)\n",
    "    val_loss = random.uniform(0.2, 1.0) / (epoch + 1.5)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss\n",
    "    })\n",
    "\n",
    "    # Simulate time taken for each epoch\n",
    "    time.sleep(1)\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Metrics and Visualizations\n",
    "Log training metrics, custom visualizations, images, and tables to track experiment progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log custom visualizations, images, and tables\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Log a custom line plot\n",
    "epochs = list(range(1, config.epochs + 1))\n",
    "accuracy = [random.uniform(0.7, 0.9) for _ in epochs]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, accuracy, marker='o', label=\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot as an image and log it to W&B\n",
    "plt.savefig(\"accuracy_plot.png\")\n",
    "wandb.log({\"accuracy_plot\": wandb.Image(\"accuracy_plot.png\")})\n",
    "plt.close()\n",
    "\n",
    "# Log an example image\n",
    "image_array = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n",
    "image = Image.fromarray(image_array)\n",
    "image.save(\"example_image.png\")\n",
    "wandb.log({\"example_image\": wandb.Image(\"example_image.png\")})\n",
    "\n",
    "# Log a table with sample data\n",
    "data = {\n",
    "    \"Parameter\": [\"Learning Rate\", \"Batch Size\", \"Epochs\"],\n",
    "    \"Value\": [config.learning_rate, config.batch_size, config.epochs]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "wandb.log({\"experiment_parameters\": wandb.Table(dataframe=df)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts and Model Versioning\n",
    "Create and version artifacts like datasets and models, implement model registry practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and log an artifact for a dataset\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"sample-dataset\",  # Name of the artifact\n",
    "    type=\"dataset\",  # Type of the artifact\n",
    "    description=\"A sample dataset for demonstration purposes\",  # Description\n",
    "    metadata={\"source\": \"synthetic\"}  # Additional metadata\n",
    ")\n",
    "\n",
    "# Save a sample dataset to a CSV file\n",
    "sample_data = pd.DataFrame({\n",
    "    \"feature1\": np.random.rand(100),\n",
    "    \"feature2\": np.random.rand(100),\n",
    "    \"label\": np.random.randint(0, 2, 100)\n",
    "})\n",
    "sample_data.to_csv(\"sample_dataset.csv\", index=False)\n",
    "\n",
    "# Add the dataset file to the artifact\n",
    "artifact.add_file(\"sample_dataset.csv\")\n",
    "\n",
    "# Log the artifact to W&B\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "# Create and log an artifact for a model\n",
    "model_artifact = wandb.Artifact(\n",
    "    name=\"sample-model\",  # Name of the artifact\n",
    "    type=\"model\",  # Type of the artifact\n",
    "    description=\"A sample model for demonstration purposes\",  # Description\n",
    "    metadata={\"framework\": \"scikit-learn\"}  # Additional metadata\n",
    ")\n",
    "\n",
    "# Save a sample model (e.g., a dummy file for demonstration)\n",
    "with open(\"sample_model.pkl\", \"w\") as f:\n",
    "    f.write(\"This is a placeholder for a model file.\")\n",
    "\n",
    "# Add the model file to the artifact\n",
    "model_artifact.add_file(\"sample_model.pkl\")\n",
    "\n",
    "# Log the model artifact to W&B\n",
    "wandb.log_artifact(model_artifact)\n",
    "\n",
    "# Use the W&B Model Registry to version the model\n",
    "# Fetch the latest version of the model artifact\n",
    "artifact_version = wandb.use_artifact(\"sample-model:latest\")\n",
    "\n",
    "# Download the artifact files\n",
    "artifact_dir = artifact_version.download()\n",
    "\n",
    "# Print the path to the downloaded artifact files\n",
    "print(f\"Artifact downloaded to: {artifact_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Sweeps\n",
    "Define sweep configurations, run hyperparameter optimization experiments, and analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",  # Use grid search for hyperparameter optimization\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",  # Optimize for validation loss\n",
    "        \"goal\": \"minimize\"  # Minimize the validation loss\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001, 0.01, 0.1]  # Test different learning rates\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [16, 32, 64]  # Test different batch sizes\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"value\": 5  # Fix the number of epochs\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"mlops-introduction\")\n",
    "\n",
    "# Define the training function\n",
    "def train_model():\n",
    "    # Initialize a new W&B run\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config  # Access the sweep configuration\n",
    "\n",
    "        # Simulate a training loop\n",
    "        for epoch in range(config.epochs):\n",
    "            # Simulate training and validation loss\n",
    "            train_loss = random.uniform(0.2, 1.0) / (epoch + 1)\n",
    "            val_loss = random.uniform(0.2, 1.0) / (epoch + 1.5)\n",
    "\n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss\n",
    "            })\n",
    "\n",
    "# Run the sweep agent\n",
    "wandb.agent(sweep_id, function=train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with ML Frameworks\n",
    "Integrate wandb with popular frameworks like TensorFlow, PyTorch, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration with TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a simple TensorFlow model\n",
    "tf_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(10,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "tf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Log model summary to W&B\n",
    "wandb.log({\"model_summary\": tf_model.summary()})\n",
    "\n",
    "# Train the model and log metrics to W&B\n",
    "tf_model.fit(\n",
    "    x=np.random.rand(100, 10),  # Random training data\n",
    "    y=np.random.randint(0, 2, 100),  # Random labels\n",
    "    batch_size=config.batch_size,\n",
    "    epochs=config.epochs,\n",
    "    callbacks=[wandb.keras.WandbCallback()]  # W&B callback for TensorFlow\n",
    ")\n",
    "\n",
    "# Integration with PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple PyTorch model\n",
    "class PyTorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "pytorch_model = PyTorchModel()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(pytorch_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Train the PyTorch model and log metrics to W&B\n",
    "for epoch in range(config.epochs):\n",
    "    inputs = torch.rand(100, 10)  # Random training data\n",
    "    labels = torch.randint(0, 2, (100, 1)).float()  # Random labels\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = pytorch_model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\"epoch\": epoch + 1, \"loss\": loss.item()})\n",
    "\n",
    "# Integration with scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a simple scikit-learn model\n",
    "sklearn_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Generate random training data\n",
    "X_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Train the model\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate random test data\n",
    "X_test = np.random.rand(20, 10)\n",
    "y_test = np.random.randint(0, 2, 20)\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "y_pred = sklearn_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Log metrics to W&B\n",
    "wandb.log({\"sklearn_accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Collaboration Features\n",
    "Explore team projects, shared dashboards, and collaboration workflows for MLOps teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore team projects, shared dashboards, and collaboration workflows for MLOps teams\n",
    "\n",
    "# Create a new team project\n",
    "team_project = wandb.init(\n",
    "    project=\"team-collaboration-demo\",  # Name of the team project\n",
    "    entity=\"your-team-name\",  # Replace with your team name\n",
    "    name=\"team-collaboration-run\"  # Name of the run\n",
    ")\n",
    "\n",
    "# Log a shared dashboard\n",
    "wandb.log({\n",
    "    \"dashboard_link\": \"https://wandb.ai/your-team-name/team-collaboration-demo\"  # Replace with your actual dashboard link\n",
    "})\n",
    "\n",
    "# Simulate a collaborative workflow\n",
    "# Log metrics from multiple team members\n",
    "team_metrics = [\n",
    "    {\"member\": \"Alice\", \"accuracy\": 0.85, \"loss\": 0.4},\n",
    "    {\"member\": \"Bob\", \"accuracy\": 0.88, \"loss\": 0.35},\n",
    "    {\"member\": \"Charlie\", \"accuracy\": 0.87, \"loss\": 0.38}\n",
    "]\n",
    "\n",
    "# Log team metrics to W&B\n",
    "for metric in team_metrics:\n",
    "    wandb.log({\n",
    "        \"team_member\": metric[\"member\"],\n",
    "        \"accuracy\": metric[\"accuracy\"],\n",
    "        \"loss\": metric[\"loss\"]\n",
    "    })\n",
    "\n",
    "# Log a shared artifact for team collaboration\n",
    "team_artifact = wandb.Artifact(\n",
    "    name=\"team-shared-dataset\",\n",
    "    type=\"dataset\",\n",
    "    description=\"A dataset shared among team members for collaboration\",\n",
    "    metadata={\"team\": \"your-team-name\"}\n",
    ")\n",
    "\n",
    "# Save a shared dataset to a CSV file\n",
    "shared_data = pd.DataFrame({\n",
    "    \"feature1\": np.random.rand(50),\n",
    "    \"feature2\": np.random.rand(50),\n",
    "    \"label\": np.random.randint(0, 2, 50)\n",
    "})\n",
    "shared_data.to_csv(\"team_shared_dataset.csv\", index=False)\n",
    "\n",
    "# Add the dataset file to the artifact\n",
    "team_artifact.add_file(\"team_shared_dataset.csv\")\n",
    "\n",
    "# Log the shared artifact to W&B\n",
    "wandb.log_artifact(team_artifact)\n",
    "\n",
    "# Finish the team project run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
